\section{Implementation}

As anticipated in the introduction, this implementation aims to speed, not good
generalization, hence in order to have a good starting point, most optimizations
are done at decision tree level.

The random forest implementation works by run trees in parallel in both training
and prediction phases, avoiding race conditions (almost entirely) for every
parallel version.

\subsection{Decision Tree}

The core logic and most of the low level optimizations are implemented in the
decision tree, recursively built as a \verb|std::vector| of \verb|Node|, in
order to better exploit cache locality for faster prediction.

The tree nodes are disposed following the DFS visit order, that comes naturally
from a recursive build. To improve prediction performance, several variants have
been tested like:

\begin{itemize}
  \item \textbf{DFS with SOA}: keep the DFS layout but the tree is built following
    the SOA model, in order to improve caching, expecially for faster indexing
    of nodes.
  \item \textbf{BFS}: after building the tree, a conversion to BFS layout
    were performed, in order to perform better (at least on average) than the
    DFS layout for prediction.
\end{itemize}

Both this variants didn't bring any improvement to the prediction phase,
probably due to the fact that the DFS current tree is compact enough to fit in
the cache. So in order to keep the code cleaner and do not add the cost of a BFS
conversion the choice end up being a simple DFS layout.

The training algorithm finds the optimal split by considering every feature, and
for each of them computing an optimal split threshold using information gain and
entropy as metrics:

\begin{enumerate}
  \item Sort the feature by creating indices to redirect the indexing (dataset
    is immutable).
  \item If there is a change of class, the mid point between the previous and
    current feature value is examined as candidate threshold.
  \item If the candidate threshold has an higher information gain than the
    previous one, then it becomes the new best threshold candidate.
\end{enumerate}

In this phase, most of the optimizations and time improvements are due to a
column-major memory layout, obtained by creating a transposed copy of the
original \emph{samples} $\times$ \emph{features} matrix. In this way is possible
to exploit cache locality while scanning every feature value, searching for an
optimal split.

Another key optimization comes by considering the following entropy and
information gain formulations:

\begin{align*}
  \text{Entropy}(S) &= -\sum_{i=1}^n p_i \log_2(p_i) \\
  \text{Gain}(S, A) &= \text{Entropy}(S) - \sum_{v \in \text{Values}(A)}
  \frac{\#(Sv)}{\#(S)} \cdot \text{Entropy}(Sv)
\end{align*}

where $S$ is the consider set of samples, $A$ is the considered attribute (or
feature) and $Sv$ is the subset of $S$ with attribute $A$ having value $v$. The
first term of this formulation can be computed once for every feature and then
riused every time there is the need to analyze a new candidate threshold.

Another related optimization exploits the fact that the considered feature is
sorted, therefore is possible to think that the loop "moves" a boundary along
the considered feature axis, gradually dividing encountered points in
\emph{left} and \emph{right} subsets. This allows to count each target class
cardinality once at the beginning and consider all the points as belonging to
the \emph{right} subset. The loop just update the counter by decreasing
\emph{right} counters on the and increasing \emph{left} counters, avoiding to
completely rescan the target column each time \verb|informationGain| and
\verb|entropy| functions are invoked.

% leave for later if implementing BFS
For the prediction phase the main optimization comes with the tree memory
layout.

\subsection{Random Forest}

The sequential implementation of the random forest stores a set of decision
trees and train them sequentially, each on a bootstrap sampling of the original
dataset.

For the prediction phase each tree performs a batch prediction on the given
input returning a vector of labels. The vector is then used to increased a
counters matrix that has a \emph{sample} $\times$ \emph{label} layout. Each row
of the matrix is a frequency vector of every possible label for an input
pattern.

Once every tree has produced its prediction, the final prediction is computed by
taking the most frequent class for each input pattern.

\subsubsection{OpenMP}

The OpenMP version uses a static scheduling for both training and prediction
phases, keeping basically the same logic of the sequential version.

While the training phase uses a parallel for to parallelize each tree training,
the prediction phase is splitted in two parallel steps. In the first step each
tree produces its prediction and stores it in shared matrix with a \emph{tree}
$\times$ \emph{sample} layout. There is no need of synchronization since the
shared structure is projected for each tree to write its own cell.

After producing predictions, the second parallel phase updates a matrix of
counters, as before with a \emph{sample} $\times$ \emph{label} layout, and
computes the majority vote for the final prediction.

Again no synchronization is needed due to the fact that every shared data
structure is accessed exploiting the independence of each operation.

\subsubsection{FastFlow}

For the FastFlow version the main logic remains the same as the OpenMP version
but instead of parallel for, the selected mechanism is the \verb|ff_Farm|.

For the training part each worker has its own reference of the vector of trees,
and the dataset. A \verb|Source| node only dispatches the index of the tree that
the worker should train.

The prediction part uses two independent farms that implement the exact same
logic used for the OpenMP version, and again using a \verb|Source| node in
charge of dispatching indices.

This simple design choice was the first tried, yet the best one found, also
capable of exposing worker tasks in their minimal form.

\subsubsection{MPI}

The distributed hybrid implementation combines MPI and OpenMP to exploit both
multiple nodes and low-latency shared memory. OpenMP was chosen over FastFlow
because it better aligns with the data-parallel paradigm required by this task
and modelled by the MPI implementation of the forest.

About the actual implementation of MPI the choice was a minimal yet efficient
architecture, where communications among processes are limited as much as
possible.

For the training phase the algorithm exploits the fact that the main program is
replicated from start to finish and so there is no need for communication at
all. Every process trains \emph{total\_trees} $/$ \emph{workers} decision trees
obtaining the number of total processes by calling \verb|MPI_Comm_size|.

For the prediction phase is required some a minimal communication among
processes to ensure the correctness of the algorithm and it is structured in
three parts:

\begin{enumerate}
  \item  Each process produces predictions using its trees and stores them in
    flattened matrix. In particular every tree prediction over the batch of
    samples is stored contiguously, followed by the prediction array of the next
    tree.
  \item A vector of counters with dimension \emph{samples} $\times$
    \emph{labels} is used to count, for each sample the frequency of each
    possible label with the previously produced predictions.
  \item For the final part an \verb|MPI_Reduce| operation sums all the counters
    coming from each process, obtaining one final counter object.
  \item The counter can now be used to compute the majority vote for each sample
    in order to produce the final prediction.
\end{enumerate}

This design was chosen considering that the code would be executed on an
homogeneous cluster, and also assuming a fairly balanced workload among nodes.
Of course moving to an eterogeneous architecture could result in a significant
workload imbalance among nodes, that would require a dynamic scheduling policy.
