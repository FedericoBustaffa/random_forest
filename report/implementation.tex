\section{Implementation}

As anticipated in the introduction, this implementation aims to speed, not good
generalization, hence in order to have a good starting point, most optimizations
are done at decision tree level.

The random forest implementation works by run trees in parallel in both training
and prediction phases, avoiding race conditions (almost entirely) for every
parallel version.

\subsection{Decision Tree}

The core logic and most of the low level optimizations are implemented in the
decision tree, recursively built as a \verb|std::vector| of \verb|Node|,
disposed following the DFS visit order, that comes naturally from a recursive
build.

The training algorithm finds the optimal split by considering every feature, and
for each of them computing an optimal split threshold using information gain and
entropy as metrics:

\begin{enumerate}
  \item Sort the feature by creating indices to redirect the indexing (dataset
    vectors is immutable).
  \item If there is a change of class, the mid point between the previous and
    current feature value is examined as candidate threshold.
  \item If the candidate threshold has an higher information gain than the
    previous one, then it becomes the new best threshold candidate.
\end{enumerate}

In the end if the best split found is valid but with an information gain below
$10^{-6}$ a leaf node is created regardless it's entropy value being excatly
zero. Otherwise the algorithm proceeds with a recursive call on the children
nodes.

\subsection{Random Forest}

The sequential implementation of the random forest stores a set of decision
trees and train them sequentially, each on a bootstrap sampling of the original
dataset.

For the prediction phase each tree performs a batch prediction on the given
input returning a vector of labels. The vector is then used to increase a matrix
if counters that has a \emph{sample} $\times$ \emph{label} layout. Each row of
the matrix is a frequency vector containing the votes distribution of each
label predicted by all trees for a single input pattern.

Once every tree has produced its prediction, the final prediction is computed by
taking the most frequent class for each input pattern.

Moving to the parallelization choices and design, only the forest is
parallelized; this choice was made considering the usual number of estimators
for the examined datasets and the total number of processors available on the
cluster. Parallelizing trees involves a nested parallelization paradigm, more
difficult to handle and possibly worse in performance due to overhead.


\subsubsection{OpenMP}

The OpenMP version uses a static scheduling for both training and prediction
phases, keeping basically the same logic of the sequential version.

While the training phase uses a parallel for to parallelize each tree training,
the prediction phase is splitted in two parallel steps:

\begin{enumerate}
  \item Each tree produces predictions over the batch and stores it in shared
    matrix with a \emph{tree} $\times$ \emph{sample} layout.
  \item Update the matrix of counters, parallelizing on the number of samples
    and compute the majority vote for each row.
\end{enumerate}

For both phases no synchronization is needed, since every computation and data
structure modification is completely independent.

\subsubsection{FastFlow}

For the FastFlow version the main logic remains the same as the OpenMP version
but instead of parallel for, the selected mechanism is the \verb|ff_Farm|.

For the training part each worker has its own reference of the vector of trees,
and the dataset. A \verb|Source| node only dispatches the indices of the trees
that the worker should train.

The prediction part uses two independent farms that implement the exact same
logic used for the OpenMP version, and again using a \verb|Source| node in
charge of dispatching indices.

This simple design choice was the first tried, yet the best one found, also
capable of exposing worker tasks in their minimal form.

\subsubsection{MPI}

The distributed hybrid implementation combines MPI and OpenMP to exploit both
multiple nodes and low-latency shared memory. OpenMP was chosen over FastFlow
because it better aligns with the data-parallel paradigm required by this task
and modelled by the MPI implementation of the forest.

Every process trains \emph{total\_trees} $/$ \emph{workers} decision trees
obtaining the number of total processes by calling \verb|MPI_Comm_size|.

For the prediction phase is required a minimal communication among
processes to ensure the correctness of the algorithm. The logic is almost
identical with the OpenMP version, where every process parallelizes over its
trees the prediction and fill a flattened matrix of counters as before.

Every process shares its vote counter matrix with an \verb|MPI_Reduce| towards
MPI rank 0 process, summing all the votes. In the end only rank 0 perform a
majority vote, producing the final prediction.

