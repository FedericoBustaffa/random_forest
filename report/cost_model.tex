\section{Cost Model}

Let's define the cost model starting from the single decision tree training and
prediction phases by defining $N$ be the number of samples, $F$ the number of
features and $L$ the number of labels.

For the training phase the heavier part is indeed the loop to search the optimal
split that involves:

\begin{itemize} 
  \item Feature sorting: $\O(N \log N)$
  \item Feature scan: $\O (N)$
  \item Threshold evaluations: $\O (L)$
\end{itemize}

that in total results in a cost per node of

\[ \O (F \cdot (N \log N + N \cdot L)) \]

resulting in a full training cost that could be proportional to the number of
samples in the worst case scenario:

\begin{align*}
  \mathcal{T}_t &= \O (N \cdot F \cdot (N \log N + N \cdot L)) \\
              &= \O (N \cdot F \cdot N \cdot (\log N + L)) \\
              &= \O (N^2 \cdot F \cdot (\log N + L))
\end{align*}

For the prediction phase we have that every sample pays the cost of a DFS visit
that is proportional to the tree depth ($D$), resulting in a total cost over a
batch of samples of

\[ \mathcal{P}_t = \O (N \cdot D) \]

For the distributed version, supposing a forest composed of $T$ trees, and $P$
MPI processes, every process takes $T / P$ trees and reads the entire dataset,
creating its own copy. Considering also $W$ as the number of worker threads
involved and the fact that there is no communication, the total cost of
training is

\[ \mathcal{T}_d = \O \left( \frac{T}{P \cdot W} \cdot \mathcal{T}_t \right) \]

For the prediction phase it's necessary to pay the cost of a sum reduction to
aggregate all the votes before the rank 0 is able to compute the majority.
Considering that every node produces predictions only with its trees, the per
node cost of prediction is roughly

\[ \O \left( \frac{T}{P \cdot W} \cdot \mathcal{P}_t \right) \]

To this must be added the reduction cost of sum reduction that, considering the
size of the vector of votes being $N \times L$, should be

\[ \O \left( \log_2 P \cdot (N \cdot L) \right) \]

resulting in a final prediction cost of

\[
  \mathcal{P}_d = \O \left( \frac{T}{P \cdot W} \cdot \mathcal{P}_t + \log_2 P
  \cdot (N \cdot L) \right)
\]

\subsection{Bottlenecks}

The current decision tree implementation, evaluated with a performance profiler,
seems to spend most of the training time sorting all the features at each split.
This could be mitigated with a global pre-sorting of every feature, then at
every split, a mask or some sort of indexing could be applied, removing uneeded
values.

Let's also point out that the model rivals Scikit-Learn implementation, that
could use this kind of optimizations, showing no urgence real need to implement
it.

For the distributed version, the current training design assumes homogeneous
cluster of nodes and a well-balanced workload for each of them. Moving to an
eterogeneous architecture could result in a significant workload imbalance among
nodes. In case there would be the need of a dynamic scheduling policy or the
bottleneck will be fore sure the slowest node.

Another aspect is indeed the distributed prediction phase, that shown no
improvements moving on multi-node architecture, on the countrary it got worse in
many cases.

This can be imputed to an excessive tree-wise parallelization, for example with
8 nodes and 32 threads per node, a random forest with 256 trees will have a tree
per core, and if the samples are not enough or the trees are small the
prediction time of a single tree is very low.

\subsection{Optimizations}


