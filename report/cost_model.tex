\section{Cost Model}

Let's define the cost model starting from the single decision tree training and
prediction phases by defining $N$ be the number of samples, $F$ the number of
features and $L$ the number of labels.

For the training phase the heavier part is indeed the loop to search the optimal
split that involves:

\begin{itemize} 
  \item Feature sorting: $\O(N \log N)$
  \item Feature scan: $\O (N)$
  \item Threshold evaluations: $\O (L)$
\end{itemize}

that in total results in a cost per node of

\[ \O (F \cdot (N \log N + N \cdot L)) \]

resulting in a full training cost that could be proportional to the number of
samples in the worst case scenario:

\begin{align*}
  \mathcal{T}_t &= \O (N \cdot F \cdot (N \log N + N \cdot L)) \\
              &= \O (N \cdot F \cdot N \cdot (\log N + L)) \\
              &= \O (N^2 \cdot F \cdot (\log N + L))
\end{align*}

For the prediction phase we have that every sample pays the cost of a DFS visit
that is proportional to the tree depth ($D$), resulting in a total cost over a
batch of samples of

\[ \mathcal{P}_t = \O (N \cdot D) \]

For the distributed version, supposing a forest composed of $T$ trees, and $P$
MPI processes, every process takes $T / P$ trees and reads the entire dataset,
creating its own copy. Considering also $W$ as the number of worker threads
involved and the fact that there is no communication, the total cost of
training is

\[ \mathcal{T}_d = \O \left( \frac{T}{P \cdot W} \cdot \mathcal{T}_t \right) \]

For the prediction phase it's necessary to pay the cost of a sum reduction to
aggregate all the votes before the rank 0 is able to compute the majority.
Considering that every node produces predictions only with its trees, the per
node cost of prediction is roughly

\[ \O \left( \frac{T}{P \cdot W} \cdot \mathcal{P}_t \right) \]

To this must be added the reduction cost of sum reduction that, considering the
size of the vector of votes being $N \times L$, should be

\[ \O \left( \log_2 P \cdot (N \cdot L) \right) \]

resulting in a final prediction cost of

\[
  \mathcal{P}_d = \O \left( \frac{T}{P \cdot W} \cdot \mathcal{P}_t + \log_2 P
  \cdot (N \cdot L) \right)
\]

\subsection{Bottlenecks}

The current decision tree implementation, evaluated with a performance profiler,
seems to spend most of the training time sorting all the features at each split.
This could be mitigated with a global pre-sorting of every feature, then at
every split, a mask or some sort of indexing could be applied, removing uneeded
values.

Let's also point out that the model rivals Scikit-Learn implementation, that
could use this kind of optimizations, showing no urgence real need to implement
it.

For the distributed version, the current training design assumes homogeneous
cluster of nodes and a well-balanced workload for each of them. Moving to an
eterogeneous architecture could result in a significant workload imbalance among
nodes. In case there would be the need of a dynamic scheduling policy or the
bottleneck will be fore sure the slowest node.

Another aspect is indeed the distributed prediction phase, that shown no
improvements moving on multi-node architecture, on the countrary it got worse in
many cases.

This can be imputed to an excessive tree-wise parallelization, for example with
8 nodes and 32 threads per node, a random forest with 256 trees will have a tree
per core, and if the samples are not enough or the trees are small the
prediction time of a single tree is very low.

\subsection{Optimizations}

The core logic and most of the low level optimizations are implemented in the
decision tree, recursively built as a \verb|std::vector| of \verb|Node|, in
order to better exploit cache locality for faster prediction.

To improve prediction performance, some variant of DFS layout have been tested
like:

\begin{itemize}
  \item \textbf{DFS with SOA}: keep the DFS layout but the tree is built following
    the SOA model, in order to improve caching, expecially for faster indexing
    of nodes.
  \item \textbf{BFS}: after building the tree, a conversion to BFS layout
    were performed, in order to perform better (at least on average) than the
    DFS layout for prediction.
\end{itemize}

Both didn't improve the prediction phase, probably due to the
fact that the DFS tree is compact enough to not cause many cache misses.

In training phase, most of the optimizations and time improvements are due to a
column-major memory layout of the dataset, obtained by creating a transposed
copy of the original \emph{samples} $\times$ \emph{features} matrix. Of course
this helps a lot the iterative process on every feature column.

Another key optimization relies on a careful reformulation of entropy and
information gain:

\begin{align*}
  \text{Entropy}(S) &= -\sum_{i=1}^n p_i \log_2(p_i) \\
  \text{Gain}(S, A) &= \text{Entropy}(S) - \sum_{v \in \text{Values}(A)}
  \frac{|S_v|}{|S|} \cdot \text{Entropy}(S_v)
\end{align*}

where $S$ denotes the current set of samples, $A$ the considered feature, and
$S_v \subseteq S$ the subset of samples whose feature $A$ takes value $v$.

When searching for the optimal split on a numerical feature, candidate
thresholds are evaluated by partitioning $S$ into two subsets, typically
referred to as \emph{left} and \emph{right}. In this scenario, the entropy of the
parent set $\text{Entropy}(S)$ is independent of the threshold and can therefore
be computed once per feature and reused for all candidate splits.

A further optimization exploits the fact that samples are sorted according to
the considered feature. Instead of recomputing class distributions from scratch
for each candidate threshold, the split point can be interpreted as a boundary
that progressively moves along the sorted feature axis. Initially, all samples
belong to the \emph{right} subset, while the \emph{left} subset is empty.

By maintaining two class-count vectors, one for the left subset and one for the
right subset, the algorithm updates these counters incrementally as the boundary
moves: when a sample crosses the boundary, its class count is decreased from the
right subset and increased in the left subset. This prefix-based strategy allows
the entropy of both subsets to be computed in constant time per threshold, using
the updated class counts.

As a result, the information gain for all candidate thresholds of a feature can
be evaluated in linear time with respect to the number of samples, avoiding
repeated scans of the target column and significantly reducing the overall
computational cost of split evaluation.

About the actual implementation of MPI the choice was a minimal yet efficient
architecture, where communications among processes are limited as much as
possible.

For the training phase the algorithm exploits the fact that the main program is
replicated from start to finish and so there is no need for communication. Every
process trains its subset of trees with bootstrap sampling.
