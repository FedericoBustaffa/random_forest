\section{Experiments and Results}

About the datasets used for testing phase, the choice went from little and
simple (to test correctness), to quite large datasets with many samples and
features, in order to evaluate how well the algorithm scales:

\begin{itemize}
  \item \textbf{Breast Cancer} \cite{breast_cancer_14}: about 500 samples with
    30 features and binary labels. Mostly used to quickly test
    correctness.
  \item \textbf{Magic Gamma Telescope} \cite{magic_gamma_telescope_159}: about
    19.000 samples with 10 continuous features and binary labels. This was the
    main dataset used for medium workload performance tests.
  \item \textbf{SUSY} \cite{susy_279}: about 5.000.000 samples with 18
    continuous features and binary labels. Of course two subsets of the dataset
    have been created, one of 20.000 samples for comparison with Magic Gamma
    Telescope, and the other one of 100.000 samples.
\end{itemize}

Most tests done on the cluster look for performance, except an initial
correctness check for both decision trees and random forest done by comparing
results with Scikit-Learn.

\subsection{Correctness}

In order to compare decision tree and random forest with Scikit-Learn
implementations in a fair way, Scikit-Learn implementations have been set to
consider all the feature at every split and without limiting the maximum depth.

For the decision tree comparing the reached depth was also a way to see if the
proposed implementation evaluates every split and produces roughly the same
amount of nodes as a state of the art implementation.


\begin{table}[H]
  \centering
  \begin{tabular}{ll|rrrrr}
    \toprule
    Algorithm & Dataset & Depth & Accuracy & F1 & Training & Prediction \\
    \midrule
    Sklearn & Breast Cancer & 7 & 0.95 & 0.94 & 12.57 ms & 0.37 ms \\
    Proposed & Breast Cancer & 7 & 0.92 & 0.91 & 4.75 ms & 0.004 ms \\
    Sklearn & Magic & 26 & 0.82 & 0.80 & 0.49 s & 1.00 ms \\
    Proposed & Magic & 32 & 0.83 & 0.82 & 0.22 s & 0.43 ms \\
    Sklearn & SUSY 20k & 36 & 0.71 & 0.71 & 0.97 s & 1.67 ms \\
    Proposed & SUSY 20k & 33 & 0.71 & 0.71 & 0.44 s & 0.49 ms \\
    Sklearn & SUSY 100k & 41 & 0.72 & 0.71 & 6.29 s & 6.82 ms \\
    Proposed & SUSY 100k & 45 & 0.71 & 0.71 & 3.17 s & 3.72 ms \\
    \bottomrule
  \end{tabular}
  \caption{Comparison between the proposed decision tree implementation and
  Scikit-Learn}
  \label{tab: tree_comparison}
\end{table}

The correctness test simply consisted in a comparison of accuracy and f1 score,
on the same test split. Aside from correctness, a useful comparison with
Scikit-Learn was also about runtime, in order to have a performance goal to
reach.

For random forest, every version correctness is tested in order to ensure that
parallelization does not corrupt the algorithm; of course little score
discrepancies can be imputed to slightly different implementations.

\begin{table}[H]
  \centering
  \begin{tabular}{llll | rr}
    \toprule
    Backend & Threads & Nodes & Dataset & Accuracy & F1 \\
    \midrule
    Sequential & 1 & 1 & Breast Cancer & 0.96 & 0.96 \\
    OpenMP & 16 & 1 & Breast Cancer & 0.96 & 0.96 \\
    FastFlow & 16 & 1 & Breast Cancer & 0.96 & 0.96 \\
    MPI & 16 & 8 & Breast Cancer & 0.96 & 0.95 \\
    Sklearn & 16 & 1 & Breast Cancer & 0.96 & 0.96 \\
    \midrule
    Sequential & 1 & 1 & Magic & 0.87 & 0.86 \\
    OpenMP & 16 & 1 & Magic & 0.87 & 0.86 \\
    FastFlow & 16 & 1 & Magic & 0.87 & 0.86 \\
    MPI & 16 & 8 & Magic & 0.87 & 0.85 \\
    Sklearn & 16 & 1 & Magic & 0.88 & 0.86 \\
    \midrule
    Sequential & 1 & 1 & SUSY 20k & 0.80 & 0.80 \\
    OpenMP & 16 & 1 & SUSY 20k & 0.80 & 0.80 \\
    FastFlow & 16 & 1 & SUSY 20k & 0.80 & 0.80 \\
    MPI & 16 & 8 & SUSY 20k & 0.79 & 0.79 \\
    \bottomrule
  \end{tabular}
  \caption{Comparison between the proposed random forest implementation and
  Scikit-Learn with 128 estimators}
  \label{tab: forest_comparison}
\end{table}

Results on SUSY dataset with 100.000 samples are not reported, since accuracy
and F1 score were the same as the subset of 20.000 samples.

\subsection{Shared Memory}

Shared memory versions were tested with an exponentially growing number of trees
(from 8 to 256) and threads (from 1 to 32), taking the number of trees as a
measure of the problem size. Below are reported the execution time plots on
Magic and SUSY (20.000 samples) datasets, where executions denoted with a single
thread represent the pure sequential version.

% \begin{figure}[H]
%   \centering
%
%   \begin{subfigure}{\linewidth}
%     \centering
%     \includesvg[width=\linewidth]{images/magic_runtime.svg}
%   \end{subfigure}
%
%   \vspace{0.5cm}
%
%   \begin{subfigure}{\linewidth}
%     \centering
%     \includesvg[width=\linewidth]{images/susy_runtime.svg}
%   \end{subfigure}
%
%   \caption{Training and prediction runtime varying the number of workers on the
%   Magic and SUSY datasets.}
%   \label{fig: runtime_workers}
% \end{figure}


\begin{figure}[H]
  \centering
  \includesvg[width=\linewidth]{images/magic_runtime.svg}
  \caption{Training and prediction runtime on Magic dataset.}
  \label{fig: magic_runtime_workers}
\end{figure}

\begin{figure}[H]
  \centering
  \includesvg[width=\linewidth]{images/susy_runtime.svg}
  \caption{Training and prediction runtime on SUSY dataset.}
  \label{fig: susy_runtime_workers}
\end{figure}

As shown in the plots, both OpenMP and FastFlow version behave as expected with
a gradually decreasing execution training time as the number of threads
increases. For the prediction is possible to see how the OpenMP version keeps
decreasing also for small workloads, while the FastFlow version, expecially with
32 threads, gets worse than the 16 threads runs.

From those execution times the derived speedup values for each version are
computed with respect to the sequential version.

% \begin{figure}[H]
%   \centering
%
%   \begin{subfigure}{\linewidth}
%     \centering
%     \includesvg[width=\linewidth]{images/magic_speedup.svg}
%   \end{subfigure}
%
%   \begin{subfigure}{\linewidth}
%     \centering
%     \includesvg[width=\linewidth]{images/susy_speedup.svg}
%   \end{subfigure}
%
%   \caption{Training and prediction speedup on Magic and SUSY datasets.}
%   \label{fig: speedup}
% \end{figure}


\begin{figure}[H]
  \centering
  \includesvg[width=\linewidth]{images/magic_speedup.svg}
  \caption{Training and prediction speedup on Magic dataset.}
  \label{fig: magic_speedup}
\end{figure}

\begin{figure}[H]
  \centering
  \includesvg[width=\linewidth]{images/susy_speedup.svg}
  \caption{Training and prediction speedup on SUSY dataset.}
  \label{fig: susy_speedup}
\end{figure}

As shown the OpenMP version is very close to the ideal speedup, expecially in
training. For the FastFlow version instead was recorded an unexpected behavior,
as plots show how runs with lower workloads benefit more of parallelism in
training. The other interesting aspect is how with 32 threads there is a
stronger grow in speedup with respect to the rest of the curve trend.

By comparing the obtained speedup with the amount of used resources it was
computed the efficiency of both versions.

\begin{figure}[H]
  \centering
  \includesvg[width=\linewidth]{images/magic_efficiency.svg}
  \caption{Training and prediction efficiency on Magic dataset.}
  \label{fig: magic_efficiency}
\end{figure}

\begin{figure}[H]
  \centering
  \includesvg[width=\linewidth]{images/susy_efficiency.svg}
  \caption{Training and prediction efficiency on SUSY dataset.}
  \label{fig: susy_efficiency}
\end{figure}

For efficiency is possible to see how OpenMP version is more stable and with
much higher efficiency in both training and prediction phase. For FastFlow
efficiency decreases faster, still reflecting the strange behavior pointed out
for the speedup, showing an higher efficiency using 32 threads instead of 16.

The last metric is the weak scaling, computed as relative speedup with respect
to the previous configuration \emph{workers-estimators}. In this way is possible
to have a normalized weak speedup value with ideal value 1.

% \begin{figure}[H]
%   \centering
%   \includesvg[width=\linewidth]{images/magic_weakscaling.svg}
%   \caption{Magic dataset training and prediction weak scaling varying the number
%   of workers}
%   \label{fig: magic_weak_scaling}
% \end{figure}
%
%
% \begin{figure}[H]
%   \centering
%   \includesvg[width=\linewidth]{images/susy_weakscaling.svg}
%   \caption{Training and prediction weak scaling on SUSY dataset.}
%   \label{fig: susy_weak_scaling}
% \end{figure}


\begin{figure}[H]
  \centering
  \includesvg[width=\linewidth]{images/magic_weakscaling_norm.svg}
  \caption{Training and prediction weak scaling on Magic dataset.}
  \label{fig: susy_weak_scaling}
\end{figure}


\begin{figure}[H]
  \centering
  \includesvg[width=\linewidth]{images/susy_weakscaling_norm.svg}
  \caption{Training and prediction weak scaling on SUSY dataset.}
  \label{fig: magic_weak_scaling}
\end{figure}

For the weak scaling is possible to see how again OpenMP obtain quite stable
values for both training and prediction. FastFlow version instead has a much
noisier curve, expecially in prediction phase, probably due to an excessive
overhead for communication, that initally worsens perforamnce with respect to
the sequential version. After the point the curve has a much standard trend.

\subsection{Distributed Memory}

For the distributed memory version with MPI and OpenMP the reported results
consider a minimum of 8 threads per node, up to a maximum of 32, leaving the
number of trees fixed to 128 for most of the metrics.

