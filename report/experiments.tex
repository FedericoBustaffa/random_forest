\section{Experiments and Results}

About the datasets used for testing phase, the choice went from little and
simple (to test correctness), to quite large datasets with many samples and
features, in order to evaluate how well the algorithm scales:

\begin{itemize}
  \item \textbf{Breast Cancer} \cite{breast_cancer_14}: about 500 samples with
    30 features and binary labels. Mostly used to quickly test
    correctness.
  \item \textbf{Magic Gamma Telescope} \cite{magic_gamma_telescope_159}: about
    19.000 samples with 10 continuous features and binary labels. This was the
    main dataset used for medium workload performance tests.
  \item \textbf{SUSY} \cite{susy_279}: about 5.000.000 samples with 18
    continuous features and binary labels. Of course two subsets of the dataset
    have been created, one of 20.000 samples for comparison with Magic Gamma
    Telescope, and the other one of 100.000 samples.
\end{itemize}

Most tests done on the cluster look for performance, except an initial
correctness check for both decision trees and random forest done by comparing
results with Scikit-Learn.

Aside from correctness, a useful comparison with Scikit-Learn was also about
runtime, in order to have a performance goal to reach.


\subsection{Correctness}

In order to compare decision tree and random forest with Scikit-Learn
implementations in a fair way, Scikit-Learn implementations have been set to
consider all the feature at every split and without limiting the maximum depth.

For the decision tree comparing the reached depth was also a way to see if the
proposed implementation evaluates every split and produces roughly the same
amount of nodes as a state of the art implementation.

The correctness test simply consisted in a comparison of accuracy and f1 score,
on the same test split. 

% tabella con accuracy, f1 score e depth

For random forest, every version correctness is tested in order to ensure that
parallelization does not corrupt the algorithm; of course little score
discrepancies can be imputed to slightly different implementations.

% tabella con accuracy, f1 score

\subsection{Shared Memory}

\subsection{Distributed Memory}
