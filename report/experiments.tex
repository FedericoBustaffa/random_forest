\section{Experiments and Results}

About the datasets used for testing phase, the choice went from little and
simple (to test correctness), to quite large datasets with many samples and
features, in order to evaluate how well the algorithm scales:

\begin{itemize}
  \item \textbf{Breast Cancer} \cite{breast_cancer_14}: about 500 samples with
    30 features and binary labels. Mostly used to quickly test
    correctness.
  \item \textbf{Magic Gamma Telescope} \cite{magic_gamma_telescope_159}: about
    19.000 samples with 10 continuous features and binary labels. This was the
    main dataset used for medium workload performance tests.
  \item \textbf{SUSY} \cite{susy_279}: about 5.000.000 samples with 18
    continuous features and binary labels. Of course two subsets of the dataset
    have been created, one of 20.000 samples for comparison with Magic Gamma
    Telescope, and the other one of 100.000 samples.
\end{itemize}

Most tests done on the cluster look for performance, except an initial
correctness check for both decision trees and random forest done by comparing
results with Scikit-Learn.

\subsection{Correctness}

In order to compare decision tree and random forest with Scikit-Learn
implementations in a fair way, Scikit-Learn implementations have been set to
consider all the feature at every split and without limiting the maximum depth.

For the decision tree comparing the reached depth was also a way to see if the
proposed implementation evaluates every split and produces roughly the same
amount of nodes as a state of the art implementation.


\begin{table}[H]
  \centering
  \begin{tabular}{ll|rrrrr}
    \toprule
    Algorithm & Dataset & Depth & Accuracy & F1 & Training & Prediction \\
    \midrule
    Sklearn & Breast Cancer & 7 & 0.95 & 0.94 & 12.57 ms & 0.37 ms \\
    Proposed & Breast Cancer & 7 & 0.92 & 0.91 & 4.75 ms & 0.004 ms \\
    Sklearn & Magic & 26 & 0.82 & 0.80 & 0.49 s & 1.00 ms \\
    Proposed & Magic & 32 & 0.83 & 0.82 & 0.22 s & 0.43 ms \\
    Sklearn & SUSY 20k & 36 & 0.71 & 0.71 & 0.97 s & 1.67 ms \\
    Proposed & SUSY 20k & 33 & 0.71 & 0.71 & 0.44 s & 0.49 ms \\
    Sklearn & SUSY 100k & 41 & 0.72 & 0.71 & 6.29 s & 6.82 ms \\
    Proposed & SUSY 100k & 45 & 0.71 & 0.71 & 3.17 s & 3.72 ms \\
    \bottomrule
  \end{tabular}
  \caption{Comparison between the proposed decision tree implementation and
  Scikit-Learn}
  \label{tab: tree_comparison}
\end{table}

The correctness test simply consisted in a comparison of accuracy and f1 score,
on the same test split. Aside from correctness, a useful comparison with
Scikit-Learn was also about runtime, in order to have a performance goal to
reach.

For random forest, every version correctness is tested in order to ensure that
parallelization does not corrupt the algorithm; of course little score
discrepancies can be imputed to slightly different implementations.

\begin{table}[H]
  \centering
  \begin{tabular}{llll | rr}
    \toprule
    Backend & Threads & Nodes & Dataset & Accuracy & F1 \\
    \midrule
    Sequential & 1 & 1 & Breast Cancer & 0.96 & 0.96 \\
    OpenMP & 16 & 1 & Breast Cancer & 0.96 & 0.96 \\
    FastFlow & 16 & 1 & Breast Cancer & 0.96 & 0.96 \\
    MPI & 16 & 8 & Breast Cancer & 0.96 & 0.95 \\
    Sklearn & 16 & 1 & Breast Cancer & 0.96 & 0.96 \\
    \midrule
    Sequential & 1 & 1 & Magic & 0.87 & 0.86 \\
    OpenMP & 16 & 1 & Magic & 0.87 & 0.86 \\
    FastFlow & 16 & 1 & Magic & 0.87 & 0.86 \\
    MPI & 16 & 8 & Magic & 0.87 & 0.85 \\
    Sklearn & 16 & 1 & Magic & 0.88 & 0.86 \\
    \midrule
    Sequential & 1 & 1 & SUSY 20k & 0.80 & 0.80 \\
    OpenMP & 16 & 1 & SUSY 20k & 0.80 & 0.80 \\
    FastFlow & 16 & 1 & SUSY 20k & 0.80 & 0.80 \\
    MPI & 16 & 8 & SUSY 20k & 0.79 & 0.79 \\
    \bottomrule
  \end{tabular}
  \caption{Comparison between the proposed random forest implementation and
  Scikit-Learn with 128 estimators}
  \label{tab: forest_comparison}
\end{table}

Results on SUSY dataset with 100.000 samples are not reported, since accuracy
and F1 score were the same as the subset of 20.000 samples.

\subsection{Shared Memory}

For shared memory versions are reported absolute runtimes, speedup, efficiency
and weak scaling, also compared with Scikit-Learn implementation that offers a
single node multi-processing parallelization.

Below are reported runtime results on Magic dataset

% plot training and prediction runtime

% description

The speedup values for each version is computed with respect to the sequential
version.

% speedup plots

Efficiency is obtained by dividing the speedup for the number of workers used.

% efficiecy plots

In the end weak scaling is obtained

\subsection{Distributed Memory}
